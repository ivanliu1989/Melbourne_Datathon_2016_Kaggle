a/b
c <- a/b
c
a/15
apply(b, f(x) a/x)
apply(b, f(x){a/x})
sapply(b, f(x){a/x})
sapply(b, f(x){a/x})
lapply(b, f(x){a/x})
lapply(b, f(x)a/x)
lapply(b, f(x) a/x)
lapply(b,
f(x){
a/x
}
)
?sapply
b
sapply(b, a/x)
sapply(b, mean)
sapply(b, function(x) a/x)
a
x <âˆ’ c ( 9 : 2 0 , 1 : 5 , 3 : 7 , 0 : 8 )
x <- c( 9 : 2 0 , 1 : 5 , 3 : 7 , 0 : 8 )
x <- c(9:20,1:5,3:7,0:8)
x
(xu<-x[!duplicated(x)])
unique ( x ) # i s more e f f i c i e n t
x[!duplicated(x)]
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
rm(list=ls());gc()
library(data.table)
load('../data/model/total.RData')
source('./rscripts/0.ngram_split_func.R')
c <- unique(total$class_description)
print(c)
c <- unique(total$class_description)[1]
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=10, text_vector, grams = 2)
split_num=10
ngram_df =text_vector
grams = 2
require(RWeka)
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
cat(paste('\n (Step 2 of 5) Start to convert chunks into n-grams matrix...'))
ngram_chunks <- list()
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
chunks
head(chunks)
ngram_chunks[[1]]
ngram_chunks[[2]]
ngram_chunks[[3]]
j=1
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
split_num
ngram_chunks[[10]]
ngram_chunks[[9]]
chunks[[10]]
head(total$title)
is.na(total$title)
table(is.na(total$title))
print(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
text_vector
table(is.na(text_vector))
frequency <- ngramify(split_num=5, text_vector, grams = 2)
frequency <- ngramify(split_num=1, text_vector, grams = 2)
head(frequency)
?stemDocument
text_vectorstemDocument
text_vector
text_vector <- iconv(text_vector, to = 'utf-8', sub=' ')
review_source <- VectorSource(text_vector)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower), lazy = T)
corpus <- tm_map(corpus, removePunctuation, lazy = T)
corpus <- tm_map(corpus, stripWhitespace, lazy = T)
corpus <- tm_map(corpus, removeWords, stopwords('english'), lazy = T)
stemDocument(corpus)
?sten
?stem
?stemDocument
dtm <- DocumentTermMatrix(corpus)
dtm2 <- as.matrix(dtm)
corpus
?PlainTextDocument
PlainTextDocument(Corpus())
PlainTextDocument(Corpus
)
PlainTextDocument(corpus)
stemDocument(PlainTextDocument(corpus))
a <- stemDocument(PlainTextDocument(corpus))
dtm <- DocumentTermMatrix(a)
a
?NGramTokenizer
text_vector <- iconv(text_vector, to = 'utf-8', sub=' ')
review_source <- VectorSource(text_vector)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower), lazy = T)
corpus <- tm_map(corpus, removePunctuation, lazy = T)
corpus <- tm_map(corpus, stripWhitespace, lazy = T)
corpus <- tm_map(corpus, removeWords, stopwords('english'), lazy = T)
corpus
summary(corpus)
corpus[1]
corpus[[1]]
corpus@Content
corpus$Content
dtm <- DocumentTermMatrix(corpus)
findFreqTerms(dtm,5)
?DocumentTermMatrix
text_vector <- iconv(text_vector, to = 'utf-8', sub=' ')
review_source <- VectorSource(text_vector)
corpus <- Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower), lazy = T)
corpus <- tm_map(corpus, removePunctuation, lazy = T)
corpus <- tm_map(corpus, stripWhitespace, lazy = T)
corpus <- tm_map(corpus, removeWords, stopwords('english'), lazy = T)
data(corpus)
corpus
data(corpus)
?writeCorpus
inspect(corpus)
?inspect
inspect(corpus)[[1]]
head(review_source)
tolower(review_source)
removePunctuation(review_source)
removePunctuation(text_vector)
t <- tolower(text_vector)
t
t <- removePunctuation(t)
t
t <- stripWhitespace(t)
t
t <- removeWords(t, stopwords('english'))
t
?removeWords
ngram_df <- t
require(RWeka)
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
cat(paste('\n (Step 2 of 5) Start to convert chunks into n-grams matrix...'))
ngram_chunks <- list()
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
rm(chunks); gc()
split_num
split_num = 1
grams
require(RWeka)
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
cat(paste('\n (Step 2 of 5) Start to convert chunks into n-grams matrix...'))
ngram_chunks <- list()
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
rm(chunks); gc()
cat(paste('\n (Step 3 of 5) Start to integrate chunks into one matrix...'))
ngram_chunks_all <- c()
for (z in 1:split_num){
ngram_chunks_all <- c(ngram_chunks_all, ngram_chunks[[z]])
}
rm(ngram_chunks); gc()
cat(paste('\n (Step 4 of 5) Start to calculate the frequency of each term...'))
ngram_freq_tb <- sort(table(ngram_chunks_all), decreasing=T)
rm(ngram_chunks_all); gc()
cat(paste('\n (Step 5 of 5) Finishing the process...'))
ngram_pred <- data.frame(terms = names(ngram_freq_tb), freq = ngram_freq_tb, row.names = NULL, stringsAsFactors = F)
rm(ngram_freq_tb); gc()
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
ngram_df = t
require(RWeka)
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
cat(paste('\n (Step 2 of 5) Start to convert chunks into n-grams matrix...'))
ngram_chunks <- list()
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
rm(chunks); gc()
cat(paste('\n (Step 3 of 5) Start to integrate chunks into one matrix...'))
ngram_chunks_all <- c()
for (z in 1:split_num){
ngram_chunks_all <- c(ngram_chunks_all, ngram_chunks[[z]])
}
rm(ngram_chunks); gc()
cat(paste('\n (Step 4 of 5) Start to calculate the frequency of each term...'))
ngram_freq_tb <- sort(table(ngram_chunks_all), decreasing=T)
rm(ngram_chunks_all); gc()
cat(paste('\n (Step 5 of 5) Finishing the process...'))
ngram_pred <- data.frame(terms = names(ngram_freq_tb), freq = ngram_freq_tb, row.names = NULL, stringsAsFactors = F)
rm(ngram_freq_tb); gc()
ngram_pred
head(ngram_pred)
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
rm(list=ls());gc()
library(data.table)
load('../data/model/total.RData')
source('./rscripts/0.ngram_split_func.R')
source('./rscripts/0.ngram_split_func.R')
c <- unique(total$class_description)[2]
print(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
text_vector
ngram_df <- tolower(ngram_df)
ngram_df = text_vector
ngram_df <- tolower(ngram_df)
ngram_df <- iconv(ngram_df, to = 'utf-8', sub=' ')
ngram_df <- tolower(ngram_df)
ngram_df <- removePunctuation(ngram_df)
ngram_df <- stripWhitespace(ngram_df)
source('./rscripts/0.ngram_split_func.R')
print(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
frequency
head(frequency)
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
rm(list=ls());gc()
library(data.table)
load('../data/model/total.RData')
source('./rscripts/0.ngram_split_func.R')
freq_list <- list()
for(c in unique(total$class_description)){
print(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
# save(frequency, file=paste0('key_word_freq_',c,'_',class_id,'.RData'))
freq_list[[class_id]] <- frequency
}
head(freq_list)
freq_list[[1120]]
freq_list[['1120']]
frequency
class_id
c
unique(total$class_description)
unique(total$class_description)[-31]
unique(total$class_description)[1]
c=unique(total$class_description)[1]
cat(paste0('/n',c))
cat(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
class_id
freq_list[[class_id]]
head(freq_list[[class_id]])
head(freq_list)
freq_list
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
rm(list=ls());gc()
library(data.table)
load('../data/model/total.RData')
source('./rscripts/0.ngram_split_func.R')
freq_list <- list()
for(c in unique(total$class_description)[-31]){
cat(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
# save(frequency, file=paste0('key_word_freq_',c,'_',class_id,'.RData'))
freq_list[[class_id]] <- frequency
}
unique(total[,c('class_id', 'class_description')])
head(freq_list[[1212]])
freq_list_2grams <- freq_list
freq_title_2grams <- freq_list
save(freq_title_2grams, file='./freq_title_2grams.RData')
freq_list <- list()
for(c in unique(total$class_description)[-31]){
cat(c)
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'title']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 3)
# save(frequency, file=paste0('key_word_freq_',c,'_',class_id,'.RData'))
freq_list[[class_id]] <- frequency
}
freq_title_3grams <- freq_list
head(freq_list[[1212]],20)
save(freq_title_3grams, file='./freq_title_3grams.RData')
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
rm(list=ls());gc()
library(data.table)
load('../data/model/total.RData')
load('./freq_title_2grams.RData')
head(freq_list[[1212]],100)
head(freq_title_2grams[[1212]],100)
myRows <- c()
for (i in head(freq_title_2grams[[1212]],50)){
myRows <- c(grep(pattern="barista",x=total$title,ignore.case = TRUE), myRow)
}
myRows <- c()
for (i in head(freq_title_2grams[[1212]],50)){
myRows <- c(grep(pattern="barista",x=total$title,ignore.case = TRUE), myRows)
}
myRows
i
head(freq_title_2grams[[1212]],50)
head(freq_title_2grams[[1212]],50)$terms
myRows <- c()
for (i in head(freq_title_2grams[[1212]],50)$terms){
myRows <- c(grep(pattern="barista",x=total$title,ignore.case = TRUE), myRows)
}
i
myRows <- unique(myRows)
myRows
total$prediction <- 0
total[myRows,'prediction'] <- 1
trainRows <- which(total$hat != -1)
scoreRows <- which(total$hat == -1)
library(caTools)
act <- jobs[trainRows,'hat']
act <- total[trainRows,'hat']
pred <- total[trainRows,'prediction']
AUC <- colAUC(pred,act,plotROC=TRUE)[1]
AUC
GINI <- 2 * (AUC - 0.5)
GINI
myRows <- c()
for (i in head(freq_title_2grams[[1212]],50)$terms){
cat(past(i,' \n'))
myRows <- c(grep(pattern=i,x=total$title,ignore.case = TRUE), myRows)
}
myRows <- unique(myRows)
myRows <- c()
for (i in head(freq_title_2grams[[1212]],50)$terms){
cat(paste(i,' \n'))
myRows <- c(grep(pattern=i,x=total$title,ignore.case = TRUE), myRows)
}
myRows <- unique(myRows)
myRows <- unique(myRows)
myRows
total$prediction <- 0
total[myRows,'prediction'] <- 1
trainRows <- which(total$hat != -1)
scoreRows <- which(total$hat == -1)
#calculate the gini on train rows (these are the ones we know the answers for)
library(caTools)
act <- total[trainRows,'hat']
pred <- total[trainRows,'prediction']
AUC <- colAUC(pred,act,plotROC=TRUE)[1]
AUC
GINI <- 2 * (AUC - 0.5)
GINI
myRows <- c()
for (i in head(freq_title_2grams[[1212]],10)$terms){
cat(paste(i,' \n'))
myRows <- c(grep(pattern=i,x=total$title,ignore.case = TRUE), myRows)
}
myRows <- unique(myRows)
total$prediction <- 0
total[myRows,'prediction'] <- 1
#train and score rows
trainRows <- which(total$hat != -1)
scoreRows <- which(total$hat == -1)
#calculate the gini on train rows (these are the ones we know the answers for)
library(caTools)
act <- total[trainRows,'hat']
pred <- total[trainRows,'prediction']
AUC <- colAUC(pred,act,plotROC=TRUE)[1]
GINI <- 2 * (AUC - 0.5)
AUC
GINI
freq_title_2grams[[1212]]
head(freq_title_2grams[[1212]])
load('./freq_title_3grams.RData')
myRows <- c()
for (i in head(freq_title_3grams[[1212]],10)$terms){
cat(paste(i,' \n'))
myRows <- c(grep(pattern=i,x=total$title,ignore.case = TRUE), myRows)
}
myRows <- unique(myRows)
total$prediction <- 0
total[myRows,'prediction'] <- 1
#train and score rows
trainRows <- which(total$hat != -1)
scoreRows <- which(total$hat == -1)
#calculate the gini on train rows (these are the ones we know the answers for)
library(caTools)
act <- total[trainRows,'hat']
pred <- total[trainRows,'prediction']
AUC <- colAUC(pred,act,plotROC=TRUE)[1]
GINI <- 2 * (AUC - 0.5)
GINI
myRows <- c()
for (i in head(freq_title_3grams[[1212]],50)$terms){
cat(paste(i,' \n'))
myRows <- c(grep(pattern=i,x=total$title,ignore.case = TRUE), myRows)
}
total$prediction <- 0
total[myRows,'prediction'] <- 1
#train and score rows
trainRows <- which(total$hat != -1)
scoreRows <- which(total$hat == -1)
#calculate the gini on train rows (these are the ones we know the answers for)
library(caTools)
act <- total[trainRows,'hat']
pred <- total[trainRows,'prediction']
AUC <- colAUC(pred,act,plotROC=TRUE)[1]
GINI <- 2 * (AUC - 0.5)
head(total)
freq_list <- list()
for(c in unique(total$class_description)[-31]){
cat(paste('\n ',c,' \n '))
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'abstract']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
# save(frequency, file=paste0('key_word_freq_',c,'_',class_id,'.RData'))
freq_list[[class_id]] <- frequency
}
source('./rscripts/0.ngram_split_func.R')
freq_list <- list()
for(c in unique(total$class_description)[-31]){
cat(paste('\n ',c,' \n '))
class_id <- total[which(total$class_description == c), 'class_id'][1]
text_vector <- total[which(total$class_description == c), 'abstract']
cat(paste('Input data frame (rows:',length(text_vector), '| size:',round(object.size(text_vector)/1024/1024,0),
'mb) \n'))
frequency <- ngramify(split_num=1, text_vector, grams = 2)
# save(frequency, file=paste0('key_word_freq_',c,'_',class_id,'.RData'))
freq_list[[class_id]] <- frequency
}
text_vector
ngram_df <- text_vector
split_num=1
grams = 2
cat(paste('Input data frame (rows:',length(ngram_df), '| size:',round(object.size(ngram_df)/1024/1024,0),
'mb) \n are going to split into', split_num, 'and', grams, 'grams prediction chunks...'))
ngram_df <- iconv(ngram_df, to = 'utf-8', sub=' ')
ngram_df <- tolower(ngram_df)
ngram_df <- removePunctuation(ngram_df)
ngram_df <- stripWhitespace(ngram_df)
ngram_df <- removeWords(ngram_df, stopwords('english'))
cat(paste('\n (Step 1 of 5) Start to create chunks...'))
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
cat(paste('\n (Step 2 of 5) Start to convert chunks into n-grams matrix...'))
ngram_chunks <- list()
for (j in 1:split_num){
ngram_chunks[[j]] <- NGramTokenizer(chunks[[j]], Weka_control(min=grams,max=grams))
}
rm(chunks); gc()
split_num
chunks
chunks <- list()
for (i in 1:split_num){
chunks[[i]] <- ngram_df[(ceiling(length(ngram_df)/split_num)*(i-1)+1):(ceiling(length(ngram_df)/split_num)*i)]
}
rm(ngram_df); gc()
ngram_df <- iconv(ngram_df, to = 'utf-8', sub=' ')
ngram_df <- tolower(ngram_df)
ngram_df <- removePunctuation(ngram_df)
ngram_df <- stripWhitespace(ngram_df)
ngram_df <- removeWords(ngram_df, stopwords('english'))
ngram_df <- iconv(ngram_df, to = 'utf-8', sub=' ')
