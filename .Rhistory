# this is a little tweak so that things line up nicely later on
iris$Species <- factor(iris$Species,
levels = c("versicolor","virginica","setosa"))
head(iris)
round(cor(iris[,1:4]), 2)
pc <- princomp(iris[,1:4], cor=TRUE, scores=TRUE)
summary(pc)
plot(pc,type="lines")
biplot(pc)
library(rgl)
plot3d(pc$scores[,1:3], col=iris$Species)
text3d(pc$scores[,1:3],texts=rownames(iris))
text3d(pc$loadings[,1:3], texts=rownames(pc$loadings), col="red")
coords <- NULL
for (i in 1:nrow(pc$loadings)) {
coords <- rbind(coords, rbind(c(0,0,0),pc$loadings[i,1:3]))
}
lines3d(coords, col="red", lwd=4)
set.seed(42)
cl <- kmeans(iris[,1:4],3)
iris$cluster <- as.factor(cl$cluster)
plot3d(pc$scores[,1:3], col=iris$cluster, main="k-means clusters")
A <- matrix(c(1,2,3,4),nrow = 2, byrow = T)
B <- matrix(c(3,4,5,6),nrow = 2, byrow = T)
a <- c(1,2)
b <- c(2,3)
diag(10)
A
B
A*B
A%*%B
A%o%B # AB' A%*%t(B)
crossprod(A,B) # A'B  t(A)%*%A
crossprod(A) # A'A
b
solve(A,b) # A%*%solve(A,b)
solve(A,a) # A%*%solve(A,b)
a <- c(1200:1300)
a
b <- seq(15,100,5)
b
a/b
c <- a/b
c
a/15
apply(b, f(x) a/x)
apply(b, f(x){a/x})
sapply(b, f(x){a/x})
sapply(b, f(x){a/x})
lapply(b, f(x){a/x})
lapply(b, f(x)a/x)
lapply(b, f(x) a/x)
lapply(b,
f(x){
a/x
}
)
?sapply
b
sapply(b, a/x)
sapply(b, mean)
sapply(b, function(x) a/x)
a
x <− c ( 9 : 2 0 , 1 : 5 , 3 : 7 , 0 : 8 )
x <- c( 9 : 2 0 , 1 : 5 , 3 : 7 , 0 : 8 )
x <- c(9:20,1:5,3:7,0:8)
x
(xu<-x[!duplicated(x)])
unique ( x ) # i s more e f f i c i e n t
x[!duplicated(x)]
library(XML);
install.packages('XML')
library(XML);
url = 'http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies'
num_url<-length(url)
num_url
length(content)
xpath = 1
content=rep("text",length(xpath))
if(length(content)!=length(xpath)){
print("Error:content和xpath向量的数量不一致!")
return
}
num_vari<-length(xpath)
num_vari
result<-data.frame(rep(0,num_url))
for(i in 2:num_vari){
cbind(result,rep(0,num_url))
}
i<-1
j<-1
for(i_url in url){
i_url_parse<-htmlParse(i_url,encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
for(j in 1:num_vari){#依次填充一个页面中的不同欲读取的数据值
node<-getNodeSet(i_url_parse,xpath[j])#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[i,j]<-NA
print(paste("注意：第",j,"个变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
}else if(length(node)==1){#爬取到一个数据，说明正常
if(content[j]=="text"){#欲爬取变量的内容
result[i,j]<-xmlValue(node[[1]])
}else{#欲爬取变量的属性
result[i,j]<-xmlGetAttr(node[[1]],content[j])
result[i,j]<-iconv(result[i,j],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
}else{#爬取到多个数据，本函数不予处理
result[i,j]<-NA
print(paste("注意：第",j,"个变量能在第",i,"个页面中找到多个,不知您要哪一个，我们会把该数据写为空值"))
}
}
i<-i+1
}
i_url_parse<-htmlParse(i_url,encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
library(httr);
GET(url)
crawler1<-function(GET(url),xpath,content=rep("text",length(xpath))){
i_url <- url
i_url_parse<-htmlParse(get(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
head(i_url_parse)
i_url_parse
num_vari
node<-getNodeSet(i_url_parse,xpath[j])#通过xpath找到相应变量的xpath结点
node
xpath[j]
xmlValue(node[[1]])
node[[1]]
xmlValue(1)
?getNodeSet
readHTMLTable(rawToChar(GET(url)$content),which=1)
rawToChar(GET(url)$content)
i_url_parse
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
library(XML);
library(httr);
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
library(XML);
library(httr);
#****函数：(crawler1)
#****概要：网络抓取的主要函数1，可以抓取n个网页的m个变量。每个xpath只爬取一个数据，如果大于1个则提示有误。（精确抓取）
#****输入：
#        名称           |    数据格式
#        url            |    欲抓取的网站的url                向量：n个
#        xpath          |    给出的抓取变量的xpath            向量：m个
#        content        |    变量是结点的内容还是结点的属性值 向量：m个
#                            "text"是内容(默认)，或者是属性名称
#****输出：只有print，无输出
#        名称           |    含义
crawler1<-function(url,xpath,content=rep("text",length(xpath))){
#如果xpath以及content的数量不同，则输入数据有误
num_url<-length(url)
if(length(content)!=length(xpath)){
print("Error:content和xpath向量的数量不一致!")
return
}
#建立一个num_url行，num_vari列的数据框
num_vari<-length(xpath)
result<-data.frame(rep(0,num_url))
for(i in 2:num_vari){
cbind(result,rep(0,num_url))
}
#遍历url向量，依次对相应网页进行抓取
i<-1
j<-1
for(i_url in url){
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
for(j in 1:num_vari){#依次填充一个页面中的不同欲读取的数据值
node<-getNodeSet(i_url_parse,xpath[j])#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[i,j]<-NA
print(paste("注意：第",j,"个变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
}else if(length(node)==1){#爬取到一个数据，说明正常
if(content[j]=="text"){#欲爬取变量的内容
result[i,j]<-xmlValue(node[[1]])
}else{#欲爬取变量的属性
result[i,j]<-xmlGetAttr(node[[1]],content[j])
result[i,j]<-iconv(result[i,j],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
}else{#爬取到多个数据，本函数不予处理
result[i,j]<-NA
print(paste("注意：第",j,"个变量能在第",i,"个页面中找到多个,不知您要哪一个，我们会把该数据写为空值"))
}
}
i<-i+1
}
result
}
#****函数：(crawler2)
#****概要：网络抓取的主要函数2，可以抓取n个网页的1个变量。该xpath可以爬取多个数据，（批量抓取）
#****输入：
#        名称           |    数据格式
#        url            |    欲抓取的网站的url                向量：n个
#        xpath          |    给出的抓取变量的xpath            向量：1个
#        content        |    变量是结点的内容还是结点的属性值 向量：1个
#                            "text"是内容(默认)，或者是属性名称
#****输出：只有print，无输出
#        名称           |    含义
#        url            |    1---n自然数，相同url拥有相同数值
#        vari           |    读取的数据
crawler2<-function(url,xpath,content="text"){
num_url<-length(url)
result<-data.frame(url=0,vari=0)
i<-1#记录第几个url
tmp<-1#
for(i_url in url){
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
node<-getNodeSet(i_url_parse,xpath)#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[tmp,1]<-i
result[tmp,2]<-NA
print(paste("注意：变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
tmp<-tmp+1
}else{
for(j in 1:length(node)){
result[tmp,1]<-i
if(content=="text"){#欲爬取变量的内容
result[tmp,2]<-xmlValue(node[[j]])
}else{#欲爬取变量的属性
result[tmp,2]<-xmlGetAttr(node[[j]],content)
#result[tmp,2]<-iconv(result[tmp,2],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
tmp<-tmp+1
}
}
i<-i+1
}
result
}
url1<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies"
url2<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=2&pt=companies"
url3<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=3&pt=companies"
url<-c(url1,url2,url3)
url1<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies"
url2<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=2&pt=companies"
url3<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=3&pt=companies"
url<-c(url1,url2,url3)
xpath<-c("//a[@class='title main-headline']","//a[@class='title main-headline']","//a[@class='title main-headline']")
crawler1(url,xpath)
xpath<-c("class='title main-headline","//a[@class='title main-headline']","//a[@class='title main-headline']")
crawler1(url,xpath)
xpath<-c("//li[@class='mod result idx2 company']//div[@class='bd']//h3","//li[@class='mod result idx2 company']//div[@class='bd']//h3","//li[@class='mod result idx2 company']//div[@class='bd']//h3")
crawler1(url,xpath)
url<-"http://data.caixin.com/macro/macro_indicator_more.html?id=F0001&cpage=2&pageSize=30&url=macro_indicator_more.html#top";
xpath<-"//meta[@name='keywords']"
content<-"content"
crawler1(url,xpath,content)
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
library(XML);
library(httr);
#****函数：(crawler1)
#****概要：网络抓取的主要函数1，可以抓取n个网页的m个变量。每个xpath只爬取一个数据，如果大于1个则提示有误。（精确抓取）
#****输入：
#        名称           |    数据格式
#        url            |    欲抓取的网站的url                向量：n个
#        xpath          |    给出的抓取变量的xpath            向量：m个
#        content        |    变量是结点的内容还是结点的属性值 向量：m个
#                            "text"是内容(默认)，或者是属性名称
#****输出：只有print，无输出
#        名称           |    含义
crawler1<-function(url,xpath,content=rep("text",length(xpath))){
#如果xpath以及content的数量不同，则输入数据有误
num_url<-length(url)
if(length(content)!=length(xpath)){
print("Error:content和xpath向量的数量不一致!")
return
}
#建立一个num_url行，num_vari列的数据框
num_vari<-length(xpath)
result<-data.frame(rep(0,num_url))
for(i in 2:num_vari){
cbind(result,rep(0,num_url))
}
#遍历url向量，依次对相应网页进行抓取
i<-1
j<-1
for(i_url in url){
i_url_parse<-htmlParse(i_url,encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
for(j in 1:num_vari){#依次填充一个页面中的不同欲读取的数据值
node<-getNodeSet(i_url_parse,xpath[j])#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[i,j]<-NA
print(paste("注意：第",j,"个变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
}else if(length(node)==1){#爬取到一个数据，说明正常
if(content[j]=="text"){#欲爬取变量的内容
result[i,j]<-xmlValue(node[[1]])
}else{#欲爬取变量的属性
result[i,j]<-xmlGetAttr(node[[1]],content[j])
result[i,j]<-iconv(result[i,j],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
}else{#爬取到多个数据，本函数不予处理
result[i,j]<-NA
print(paste("注意：第",j,"个变量能在第",i,"个页面中找到多个,不知您要哪一个，我们会把该数据写为空值"))
}
}
i<-i+1
}
result
}
url<-"http://data.caixin.com/macro/macro_indicator_more.html?id=F0001&cpage=2&pageSize=30&url=macro_indicator_more.html#top";
xpath<-"//meta[@name='keywords']"
content<-"content"
crawler1(url,xpath,content)
url<-"http://list.taobao.com/itemlist/bao.htm?spm=567.116925.155171.105.9ZYYMX&cat=50072693&isprepay=1&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&smc=1&mSelect=false&user_type=0&fl=50072693#!cat=50072693&isprepay=1&user_type=0&as=0&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&mSelect=false&smc=1&json=on&tid=0"
xpath<-"//li[@class='list-item list-item-grid']"
content<-"data-commenturl"
crawler2(url1,xpath,content)
crawler2<-function(url,xpath,content="text"){
num_url<-length(url)
result<-data.frame(url=0,vari=0)
i<-1#记录第几个url
tmp<-1#
for(i_url in url){
i_url_parse<-htmlParse(i_url,encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
node<-getNodeSet(i_url_parse,xpath)#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[tmp,1]<-i
result[tmp,2]<-NA
print(paste("注意：变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
tmp<-tmp+1
}else{
for(j in 1:length(node)){
result[tmp,1]<-i
if(content=="text"){#欲爬取变量的内容
result[tmp,2]<-xmlValue(node[[j]])
}else{#欲爬取变量的属性
result[tmp,2]<-xmlGetAttr(node[[j]],content)
#result[tmp,2]<-iconv(result[tmp,2],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
tmp<-tmp+1
}
}
i<-i+1
}
result
}
url<-"http://list.taobao.com/itemlist/bao.htm?spm=567.116925.155171.105.9ZYYMX&cat=50072693&isprepay=1&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&smc=1&mSelect=false&user_type=0&fl=50072693#!cat=50072693&isprepay=1&user_type=0&as=0&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&mSelect=false&smc=1&json=on&tid=0"
xpath<-"//li[@class='list-item list-item-grid']"
content<-"data-commenturl"
crawler2(url1,xpath,content)
crawler2<-function(url,xpath,content="text"){
num_url<-length(url)
result<-data.frame(url=0,vari=0)
i<-1#记录第几个url
tmp<-1#
for(i_url in url){
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
node<-getNodeSet(i_url_parse,xpath)#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[tmp,1]<-i
result[tmp,2]<-NA
print(paste("注意：变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
tmp<-tmp+1
}else{
for(j in 1:length(node)){
result[tmp,1]<-i
if(content=="text"){#欲爬取变量的内容
result[tmp,2]<-xmlValue(node[[j]])
}else{#欲爬取变量的属性
result[tmp,2]<-xmlGetAttr(node[[j]],content)
#result[tmp,2]<-iconv(result[tmp,2],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
tmp<-tmp+1
}
}
i<-i+1
}
result
}
url<-"http://list.taobao.com/itemlist/bao.htm?spm=567.116925.155171.105.9ZYYMX&cat=50072693&isprepay=1&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&smc=1&mSelect=false&user_type=0&fl=50072693#!cat=50072693&isprepay=1&user_type=0&as=0&viewIndex=1&yp4p_page=0&commend=all&atype=b&style=grid&olu=yes&isnew=2&mSelect=false&smc=1&json=on&tid=0"
xpath<-"//li[@class='list-item list-item-grid']"
content<-"data-commenturl"
crawler2(url1,xpath,content)
"//li[@class='mod result company']"
url1<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies"
url2<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=2&pt=companies"
url3<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=3&pt=companies"
xpath<-c("//li[@class='mod result company']","//li[@class='mod result company']","//li[@class='mod result company']")
crawler1(url,xpath)
setwd('/Users/ivanliu/Downloads/datathon2016/Melbourne_Datathon_2016_Kaggle')
library(XML);
library(httr);
#****函数：(crawler1)
#****概要：网络抓取的主要函数1，可以抓取n个网页的m个变量。每个xpath只爬取一个数据，如果大于1个则提示有误。（精确抓取）
#****输入：
#        名称           |    数据格式
#        url            |    欲抓取的网站的url                向量：n个
#        xpath          |    给出的抓取变量的xpath            向量：m个
#        content        |    变量是结点的内容还是结点的属性值 向量：m个
#                            "text"是内容(默认)，或者是属性名称
#****输出：只有print，无输出
#        名称           |    含义
crawler1<-function(url,xpath,content=rep("text",length(xpath))){
#如果xpath以及content的数量不同，则输入数据有误
num_url<-length(url)
if(length(content)!=length(xpath)){
print("Error:content和xpath向量的数量不一致!")
return
}
#建立一个num_url行，num_vari列的数据框
num_vari<-length(xpath)
result<-data.frame(rep(0,num_url))
for(i in 2:num_vari){
cbind(result,rep(0,num_url))
}
#遍历url向量，依次对相应网页进行抓取
i<-1
j<-1
for(i_url in url){
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
for(j in 1:num_vari){#依次填充一个页面中的不同欲读取的数据值
node<-getNodeSet(i_url_parse,xpath[j])#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[i,j]<-NA
print(paste("注意：第",j,"个变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
}else if(length(node)==1){#爬取到一个数据，说明正常
if(content[j]=="text"){#欲爬取变量的内容
result[i,j]<-xmlValue(node[[1]])
}else{#欲爬取变量的属性
result[i,j]<-xmlGetAttr(node[[1]],content[j])
result[i,j]<-iconv(result[i,j],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
}else{#爬取到多个数据，本函数不予处理
result[i,j]<-NA
print(paste("注意：第",j,"个变量能在第",i,"个页面中找到多个,不知您要哪一个，我们会把该数据写为空值"))
}
}
i<-i+1
}
result
}
#****函数：(crawler2)
#****概要：网络抓取的主要函数2，可以抓取n个网页的1个变量。该xpath可以爬取多个数据，（批量抓取）
#****输入：
#        名称           |    数据格式
#        url            |    欲抓取的网站的url                向量：n个
#        xpath          |    给出的抓取变量的xpath            向量：1个
#        content        |    变量是结点的内容还是结点的属性值 向量：1个
#                            "text"是内容(默认)，或者是属性名称
#****输出：只有print，无输出
#        名称           |    含义
#        url            |    1---n自然数，相同url拥有相同数值
#        vari           |    读取的数据
crawler2<-function(url,xpath,content="text"){
num_url<-length(url)
result<-data.frame(url=0,vari=0)
i<-1#记录第几个url
tmp<-1#
for(i_url in url){
i_url_parse<-htmlParse(GET(i_url),encoding="UTF-8")#读取url网页数据，并使用htmlParse转化。（xml文件使用xmlParse）
node<-getNodeSet(i_url_parse,xpath)#通过xpath找到相应变量的xpath结点
if(length(node)==0){#未爬取到数据，说明xpath有误
result[tmp,1]<-i
result[tmp,2]<-NA
print(paste("注意：变量未能在第",i,"个页面中找到,我们会把该数据写为空值"))
tmp<-tmp+1
}else{
for(j in 1:length(node)){
result[tmp,1]<-i
if(content=="text"){#欲爬取变量的内容
result[tmp,2]<-xmlValue(node[[j]])
}else{#欲爬取变量的属性
result[tmp,2]<-xmlGetAttr(node[[j]],content)
#result[tmp,2]<-iconv(result[tmp,2],"UTF-8","gbk")#如果是乱码，可以打开此语句。如果是na可以删除此句
}
tmp<-tmp+1
}
}
i<-i+1
}
result
}
url1<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies"
url2<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=2&pt=companies"
url3<-"http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=3&pt=companies"
url<-c(url1,url2,url3)
xpath<-c("//li[@class='mod result company']","//li[@class='mod result company']","//li[@class='mod result company']")
crawler1(url,xpath)
xpath<-c("//li[@class='mod result idx2 company']","//li[@class='mod result company']","//li[@class='mod result company']")
crawler1(url,xpath)
xpath<-c("||li[@class='mod result idx2 company']","//li[@class='mod result company']","//li[@class='mod result company']")
crawler1(url,xpath)
i_url_parse
install.packages('RCurl')
require(RCurl)
url.exists(url="http://www.linkedin.com/vsearch/c?type=companies&orig=FCTD&rsid=1859909211461746035346&pageKey=oz-winner&trkInfo=tarId%3A1461745388674&search=Search&openFacets=N,CCR,JO,I&f_I=31,30&f_CCR=au%3A0&page_num=1&pt=companies")
d <- debugGatherer() #收集调试信息
d
tmp <- getURL(url=url, debugfunction = d$update, verbose = TRUE)
tmp
names(d$value())
cat(d$value()[1]) #服务器地址及端口号
cat(d$value()[2]) #服务器返回的头信息
cat(d$value()[3]) #提交给服务器的头信息
d$reset() # 清除d$value()
d$value() # 清除之后全部为空
h <- basicHeaderGatherer()
txt <- getURL(url=url, headerfunction = h$update)
names(h$value())
h$value()
h <- basicTextGatherer()
txt <- getURL(url, headerfunction = h$update)
names(h$value())
h$value() # 所有的内容只是一个字符串
cat(h$value()) # 用cat显示的，会比较好看
cat(h$value()) # 用cat显示的，会比较好看
curl <- getCurlHandle()
txt <- getURL(url=url, curl = curl)
names(getCurlInfo(curl))
getCurlInfo(curl)$response.code
getCurlInfo(curl=curl)
?getCurlInfo(curl=curl)
?"htmlParse"
